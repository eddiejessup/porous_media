{
 "metadata": {
  "name": "",
  "signature": "sha256:cb4fec2f1b91013dccec74fca9bd2f938a6816bbb7dab4d6cb3d428e6765627f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Food field ODE"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead of having a constant gradient field, have a field on a lattice obeying the differential equation:\n",
      "\n",
      "$\\dfrac{\\partial c(\\mathbf{x}, t)}{\\partial t} = -\\gamma \\rho(\\mathbf{x}, t) c(\\mathbf{x}, t)$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Calculation of $\\rho(x, t)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most na\u00efve way of constructing this field is,\n",
      "\n",
      "$\\rho(\\mathbf{x}, t) = \\sum_i \\prod_j \\delta(x_j - x_{i,j}(t))$\n",
      "\n",
      "where $i$ is the particle index and $j$ is the dimension index.\n",
      "\n",
      "But this only works for a field in continuous space. For one on a lattice, this becomes:\n",
      "\n",
      "$\\rho(\\mathbf{x}_k, t) = \\sum_i \\prod_j \\dfrac{H(x_{k,j} - x_{i,j}(t) - \\Delta x) - H(x_{k,j} - x_{i,j}(t) + \\Delta x)}{2 \\Delta x}$\n",
      "\n",
      "What I'm doing here is essentially the Particle-In-Cell (PIC) method. This requires the choice of the shape function, which represents how a particle is mapped onto the lattice. Read page 40 book-wise (page 46 PDF-wise) of [this](https://perswww.kuleuven.be/~u0052182/pic/book.pdf). It says that almost all PIC methods use b-splines of order zero, meaning the hat-top function above. You can use higher-order b-splines, but most don't."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Connecting $c(x, t)$ to chemotaxis machinery"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Temporal chemotaxis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This isn't too complicated, simply take $c(x, t)$ at the nearest lattice point for each particle, and add it to the chemical memory each update."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Spatial chemotaxis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this we require $\\nabla c(x, t)$. There are two obvious approaches to this:\n",
      "\n",
      "```pseudocode\n",
      "def method_1:\n",
      "    for lattice_point in field:\n",
      "        gcs[lattice_point] = grad_c(lattice_point)\n",
      "    for particle in particles:\n",
      "        lp = lattice_point(particle)\n",
      "        gc = gcs[lp]\n",
      "\n",
      "def method_2:\n",
      "    for particle in particles:\n",
      "        lp = lattice_point(particle)\n",
      "        gc = grad_c(lp)\n",
      "```\n",
      "\n",
      "Although it seems like method 2 is superior, if we have many particles on the same lattice point, it will become less efficient. If we assume a constant cost of calculating a gradient at a point, we just care about the number of gradient calculations, $n_c$\n",
      "\n",
      "For method 1: in all cases, $n_c = M^d$\n",
      "\n",
      "For method 2: in all cases, $n_c = \\mathrm{n}$\n",
      "\n",
      "Ok to be fair, in almost all cases method 2 is going to be superior. But maybe we can get the best of both worlds?\n",
      "\n",
      "```pseudocode\n",
      "def method_3:\n",
      "    for particle in particles:\n",
      "        lp = lattice_point(particle)\n",
      "        if lp in lps_seen:\n",
      "            gc = gcs[lp]\n",
      "        else:\n",
      "            gc = grad_c(lp)\n",
      "            gcs[lp] = gc\n",
      "            lps_seen += lp\n",
      "```\n",
      "\n",
      "In this case, in the best case all particles are at the same point, in which case $n_{c,\\mathrm{min}} = 1$\n",
      "\n",
      "In the worst case all particles are in different points, in which case $n_{c,\\mathrm{max}} = n$"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Results"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sensible parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's inspect the timescales in the system.\n",
      "\n",
      "1. The time for all the particles to deplete a single lattice point of food, $t_c$ to half its initial value.\n",
      "2. The persistence time of a free-swimming particle, $t_r$.\n",
      "3. The crossing time for a particle of a lattice point, $t_k$.\n",
      "\n",
      "$t_c = \\dfrac{\\log(2)}{\\gamma \\rho_\\mathrm{n}}$\n",
      "\n",
      "$\\rho_n = \\dfrac{\\mathrm{n}}{(\\Delta x)^d}$\n",
      "\n",
      "$t_c = \\dfrac{\\log(2) (\\Delta x)^d}{\\gamma \\mathrm{n}}$\n",
      "\n",
      "$t_r = \\dfrac{1}{\\mathrm{D}_{r,0}}$\n",
      "\n",
      "$t_k = \\dfrac{(\\Delta x)^2}{2 \\mathrm{D}}$\n",
      "\n",
      "$\\mathrm{D} = \\dfrac{\\mathrm{v}^2}{\\mathrm{D}_r}$\n",
      "\n",
      "$t_k = \\dfrac{\\sqrt{d} \\Delta x}{v}$\n",
      "\n",
      "If $t_c$ is too small, the food distribution will resemble a step function, and there won't be any gradient information for the particles to follow except outside of one small area.\n",
      "\n",
      "If $t_c$ is too large, only a small gradient will be formed and the particles will just diffuse apart before the initial non-uniform particle distribution can form significant chemical gradients.\n",
      "\n",
      "If $t_r$ is too small, the particles will diffuse too slowly and the food in a lattice point will be eaten before a smooth gradient can be established.\n",
      "\n",
      "If $t_r$ is too large, the particles will diffuse before the initial non-uniform particle distribution can form significant chemical gradients.\n",
      "\n",
      "If $t_k$ is too small, the gradients formed by the particles will be effectively independent, and no cohesive pattern will be seen.\n",
      "\n",
      "If $t_k$ is too large, the "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c_0 = 1.0\n",
      "gamma = 1e-6\n",
      "dx = 0.05\n",
      "d = 2\n",
      "n = 1000\n",
      "D_r = 3.0\n",
      "v = 1.0\n",
      "\n",
      "def t_c(c_0, d, dx, n, gamma):\n",
      "    return c_0 * dx ** d / (n * gamma)\n",
      "\n",
      "def t_r(D_r):\n",
      "    return 1.0 / D_r\n",
      "\n",
      "def t_k(d, dx, v):\n",
      "    return np.sqrt(d) * dx / v\n",
      "\n",
      "print('t_c ', t_c(c_0, d, dx, n, gamma))\n",
      "print('t_r ', t_r(D_r))\n",
      "print('t_k ', t_k(d, dx, v))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "t_c  2.5000000000000004\n",
        "t_r  0.3333333333333333\n",
        "t_k  0.0707106781187\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Limit on stability of a chemotaxis wave"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Assuming $D_r > 0$, there will always be some diffusion away from any chemotaxis wave.\n",
      "\n",
      "There should be a point where a wave becomes completely stable, meaning where $D_r = 0$ indefinitely. For this require the fitness induced on the enironment,\n",
      "\n",
      "$f_\\mathrm{self} > 1$\n",
      "\n",
      "$\\dfrac{\\chi}{\\mathrm{v}} \\mathbf{v} \\cdot (\\nabla c)_\\mathrm{self} > 1 \\,.$\n",
      "\n",
      "Since the wave is self-generated, the wave particles' velocities must be in the direction $(\\nabla c)_\\mathrm{self}$, of magnitude $m$,\n",
      "\n",
      "$m_\\mathrm{self} > \\dfrac{1}{\\chi} \\,.$\n",
      "\n",
      "Now to calculate what parameters are needed to create that gradient.\n",
      "\n",
      "We must calculate the amount the field is eaten by, when $\\mathrm{n}$ particles go from one end of a lattice point to another.\n",
      "\n",
      "$\\dfrac{\\partial c(\\mathbf{x}, t)}{\\partial t} = -\\gamma \\rho(\\mathbf{x}, t) c(\\mathbf{x}, t)$\n",
      "\n",
      "$\\dfrac{\\partial c(\\mathbf{x}, t)}{c(\\mathbf{x}, t)} = -\\gamma \\rho_\\mathrm{n} \\partial t$\n",
      "\n",
      "$\\int^{c_f}_{c_0} \\dfrac{\\partial c(\\mathbf{x}, t)}{c(\\mathbf{x}, t)} = -\\int^{t_k}_0 \\gamma \\rho_\\mathrm{n} \\partial t$\n",
      "\n",
      "$\\log{\\left| \\dfrac{c_f}{c_0} \\right|} = -\\gamma \\rho_\\mathrm{n} t_k$\n",
      "\n",
      "We don't need to worry about the modulus, since as long as $c_0 > 0$, our PDE ensures $c > 0$.\n",
      "\n",
      "$c_f = c_0 \\exp \\left(-\\gamma \\rho_\\mathrm{n} t_k \\right)$\n",
      "\n",
      "$\\Delta c = c_0 \\left[1 - \\exp \\left(-\\gamma \\rho_\\mathrm{n} t_k \\right) \\right]$\n",
      "\n",
      "The gradient a stable wave would see is,\n",
      "\n",
      "$m = \\dfrac{\\Delta c}{2 \\sqrt{d} \\Delta x}$\n",
      "\n",
      "(The lower factor of $2 \\sqrt{d} \\Delta x$ assumes the gradient is calculated correctly when particles are moving not along the lattice.)\n",
      "\n",
      "$m = \\dfrac{c_0 \\left[ 1 - \\exp \\left(-\\gamma \\rho_\\mathrm{n} t_k \\right) \\right]}{2 \\sqrt{d} \\Delta x}$\n",
      "\n",
      "Substituting in $t_k$ and $\\rho_\\mathrm{n}$,\n",
      "\n",
      "$m = \\dfrac{c_0 \\left[ 1 - \\exp \\left(-\\gamma \\dfrac{\\mathrm{n}}{(\\Delta x)^d} \\dfrac{\\sqrt{d} \\Delta x}{\\mathrm{v}} \\right) \\right]}{2 \\sqrt{d} \\Delta x}$\n",
      "\n",
      "So we have a condition for a stable wave to be formed,\n",
      "\n",
      "$\\dfrac{\\chi c_0 \\left[ 1 - \\exp \\left(-\\gamma \\dfrac{\\mathrm{n}}{(\\Delta x)^d} \\dfrac{\\sqrt{d} \\Delta x}{\\mathrm{v}} \\right) \\right]}{2 \\sqrt{d} \\Delta x} > 1$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gamma = 2e-4\n",
      "n = 1000\n",
      "chi = 1.0\n",
      "dx = 0.1\n",
      "v = 1.0\n",
      "gamma = 2e-4\n",
      "c_0 = 1.0\n",
      "\n",
      "def imposed_grad(c_0, gamma, rho, d, dx, v):\n",
      "    dc = c_0 * (1.0 - np.exp(-gamma * rho * t_k(d, dx, v)))\n",
      "    return dc / (2.0 * np.sqrt(d) * dx)\n",
      "\n",
      "rho_1d = n / dx\n",
      "print(imposed_grad(c_0, gamma=gamma, rho=rho_1d, d=1, dx=dx, v=v))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.90634623461\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Breakdown dynamics of a 1D wave close to the stable wave limit"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After travelling one lattice point and assuming no loss of correlation across this cell, have a gradient $m$.\n",
      "\n",
      "$m = \\dfrac{c_0 \\left[ 1 - \\exp \\left(-\\gamma \\rho_\\mathrm{n} t_k \\right) \\right]}{2 \\Delta x}$\n",
      "\n",
      "Again assuming no loss of correlation, every particle has a fitness $f = m \\chi$.\n",
      "\n",
      "Assuming spatial chemotaxis and assuming tumbling for conceptual simplicity, this means a tumbling probability $\\alpha = \\alpha_0 (1 - f) = \\alpha_0 (1 - m \\chi)$.\n",
      "\n",
      "So over the crossing time of the next lattice point, $t_k$, we expect a fraction $\\alpha t_k$ to tumble.\n",
      "\n",
      "Only half of these will result in reorientations in the opposite direction.\n",
      "\n",
      "If they reverse direction at the start of the crossing, they won't spend time in the cell, and won't deplete the food. *But* if they reverse at the end, they will contribute double, so reversal doesn't change the expected depletion in the current cell.\n",
      "\n",
      "Because we assume we're close to the stable wave limit, we can assume the tumbling probability is low, so multiple tumbles per cell are not a significant issue.\n",
      "\n",
      "Ok so by the time the bulk wave has left the cell, we have the same $\\Delta c$ as before, but now we will deplete the new cell by less because the density has decreased.\n",
      "\n",
      "The new density is expected to be $\\rho_\\mathrm{n} \\left( 1 - \\dfrac{\\alpha t_k}{2} \\right)$.\n",
      "\n",
      "We can calculate the new $m$ imposed by this density, and iterate the logic.\n",
      "\n",
      "$\\rho_\\mathrm{n, i+1} = \\rho_\\mathrm{n,i} \\left( 1 - \\dfrac{\\alpha_{i+1} t_k}{2} \\right)$\n",
      "\n",
      "$\\rho_\\mathrm{n, i+1} = \\rho_\\mathrm{n,i} \\left(1 - \\dfrac{\\alpha_0 (1 - m_{i+1} \\chi) t_k}{2} \\right)$\n",
      "\n",
      "$m_\\mathrm{i+1} = \\dfrac{c_0 \\left[ 1 - \\exp \\left(-\\gamma \\rho_\\mathrm{n, i} t_k \\right) \\right]}{2 \\Delta x}$\n",
      "\n",
      "$\\rho_\\mathrm{n, i+1} = \\rho_\\mathrm{n,i} \\left(1 - \\dfrac{\\alpha_0 (1 - \\dfrac{c_0 \\left[ 1 - \\exp \\left(-\\gamma \\rho_\\mathrm{n, i} t_k \\right) \\right]}{2 \\Delta x} \\chi) t_k}{2} \\right)$\n",
      "\n",
      "Well this is messy as fuck but it is solvable!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha_0 = 10.0\n",
      "\n",
      "dx = 0.01\n",
      "\n",
      "def breakdown_wave(rho_0, t_max, c_0, gamma, dx, v, chi, alpha_0):\n",
      "    rho = rho_1d\n",
      "    rhos = []\n",
      "    tk = t_k(d=1, dx=dx, v=v)\n",
      "    n_tk = int(t_max // tk)\n",
      "    i_tks = np.arange(n_tk)\n",
      "    for i_tk in i_tks:\n",
      "        rhos.append(rho / rho_1d)\n",
      "        m = imposed_grad(c_0, gamma, rho=rho, d=1, dx=dx, v=v)\n",
      "        f = m * chi\n",
      "        alpha = alpha_0 * (1 - f)\n",
      "        rho = rho * (1.0 - alpha * tk / 2.0)\n",
      "    ts = i_tks * tk\n",
      "    return ts, rhos\n",
      "\n",
      "for dx in [1e-2, 1e-3, 1e-4]:\n",
      "    ts, rhos = breakdown_wave(rho_1d, 3.0, c_0, gamma, dx=dx, v=v, chi=chi, alpha_0=alpha_0)\n",
      "    plt.plot(ts, rhos, label='dx = {}'.format(dx))\n",
      "plt.legend()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So it seems that a finer grained lattice means the wave persists for longer.\n",
      "\n",
      "This is because of the exponential dependence of the food field -- if the wave spends a long time in a lattice point, it will eat a significant fraction of the food in that cell, and start to see the non-linearity of the exponential term. If it's constantly switching cells, it only ever sees the linear portion, which gives the maximum gradient.\n",
      "\n",
      "We can see this by replacing the exponential solution in `imposed_grad` with the first-order linear approximation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def imposed_grad_linear(c_0, gamma, rho, d, dx, v):\n",
      "    dc = c_0 * -gamma * rho * t_k(d, dx, v)\n",
      "    return dc / (2.0 * np.sqrt(d) * dx)\n",
      "\n",
      "def breakdown_wave(rho_0, t_max, c_0, gamma, dx, v, chi, alpha_0):\n",
      "    rho = rho_1d\n",
      "    rhos = []\n",
      "    tk = t_k(d=1, dx=dx, v=v)\n",
      "    n_tk = int(t_max // tk)\n",
      "    i_tks = np.arange(n_tk)\n",
      "    for i_tk in i_tks:\n",
      "        rhos.append(rho / rho_1d)\n",
      "        m = imposed_grad_linear(c_0, gamma, rho=rho, d=1, dx=dx, v=v)\n",
      "        f = m * chi\n",
      "        alpha = alpha_0 * (1 - f)\n",
      "        rho = rho * (1.0 - alpha * tk / 2.0)\n",
      "    ts = i_tks * tk\n",
      "    return ts, rhos\n",
      "\n",
      "for dx in [1e-2, 1e-3, 1e-4]:\n",
      "    ts, rhos = breakdown_wave(rho_1d, 3.0, c_0, gamma, dx=dx, v=v, chi=chi, alpha_0=alpha_0)\n",
      "    plt.plot(ts, rhos, label='dx = {}'.format(dx))\n",
      "plt.legend()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Can see that now there's no $\\Delta x$ dependence. I'm a bit surprised the curves don't seem to converge for small $\\Delta x$ though. Need to think more about this.\n",
      "\n",
      "Actually if we're clever we can take the recursion relation in the continuous limit, and integrate. I'm not certain this is the same thing as taking $\\Delta x \\to 0$, but I'm not certain it isn't either.\n",
      "\n",
      "First, for simplicity rewrite it in terms of a minimum of constants, and rename variables,\n",
      "\n",
      "$r_\\mathrm{i+1} = r_\\mathrm{i} \\left(1 - A \\left( 1 - B \\left[ 1 - \\exp \\left(-C r_\\mathrm{i} \\right) \\right] \\right) \\right)$\n",
      "\n",
      "$r_\\mathrm{i+1} = r_\\mathrm{i} - A r_\\mathrm{i} \\left( 1 - B \\left[ 1 - \\exp \\left(-C r_\\mathrm{i} \\right) \\right] \\right)$\n",
      "\n",
      "$r_\\mathrm{i+1} - r_\\mathrm{i} = - A r_\\mathrm{i} \\left( 1 - B \\left[ 1 - \\exp \\left(-C r_\\mathrm{i} \\right) \\right] \\right)$\n",
      "\n",
      "Since this is the difference in $r$ over one $\\mathrm{i}$, this is a derivative!\n",
      "\n",
      "$\\dfrac{\\mathrm{d} r}{\\mathrm{d} i} = - A r \\left( 1 - B \\left[ 1 - \\exp \\left(-C r \\right) \\right] \\right)$\n",
      "\n",
      "Separate variables and integrate,\n",
      "\n",
      "$\\int \\dfrac{\\mathrm{d} r}{r \\left( 1 - B \\left[ 1 - \\exp \\left(-C r \\right) \\right] \\right)} = -\\int A \\mathrm{d} i$\n",
      "\n",
      "Assume we're starting from $i=0$,\n",
      "\n",
      "$\\int \\dfrac{\\mathrm{d} r}{r \\left( 1 - B \\left[ 1 - \\exp \\left(-C r \\right) \\right] \\right)} = -A i$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Anyway now I can compare this with simulations and see how it holds up.\n",
      "\n",
      "Assuming it works, we can now tinker with the parameters to see what is best to keep our wave persisting as long as possible.\n",
      "\n",
      "It might also give us an idea of where to put obstacles."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extension to a 2D linear wave"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this case, the same logic still applies, but the density must be $\\rho_\\mathrm{n}$ at every lattice point in an initial thin line. Essentially the $\\mathrm{n}$ in the above inequality becomes $\\mathrm{n} \\mathrm{M}_y$, where $\\mathrm{M}_y$ is the number of lattice points in the direction perpendicular to the wave propagation direction.\n",
      "\n",
      "$\\mathrm{M}_y = \\dfrac{L_y}{\\Delta x}$\n",
      "\n",
      "If we have $\\mathrm{n}$ points distributed uniformly between $\\mathrm{M}_y$ boxes, the mean number in each box is $\\dfrac{n}{\\mathrm{M}_y}$, and the variance I think on this also $\\dfrac{n}{\\mathrm{M}_y}$ since I think this should obey a Poisson distribution.\n",
      "\n",
      "Perhaps we can use knowledge of the noise in the distribution, to estimate the expected variation in drift speed across the wave.\n",
      "\n",
      "This may give us an estimate of the spontaneously imposed *transverse* gradient.\n",
      "\n",
      "Maybe from this we can get an idea of if and when a 2D wave becomes unstable due to these transverse gradients."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L_y = 1.0\n",
      "M = L_y / dx\n",
      "\n",
      "n = 1000\n",
      "n_per_lp = n / M\n",
      "rho_2d = n_per_lp / dx ** 2\n",
      "\n",
      "print(stability_factor(d=2, dx=dx, v=v, rho=rho_2d, gamma=gamma, chi=1.0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n"
       ]
      }
     ],
     "prompt_number": 48
    }
   ],
   "metadata": {}
  }
 ]
}